# Sorting algorithms and their complexities

## Merge sort (сортировка слиянием)
### Принцип работы алгоритма
Алгоритм **merge sort** использует принцип «разделяй и властвуй», процедура сортировки описывается следующим образом:
1. Если в рассматриваемом массиве один элемент, то он уже отсортирован — алгоритм завершает работу
2. Иначе массив разбивается на две части, которые сортируются рекурсивно
3. После сортировки двух частей массива к ним применяется **процедура слияния**, которая по двум отсортированным частям получает исходный отсортированный массив

### Процедура слияния (merge)
Пусть имеем два массива **arr1** (размер n) и **arr2** (размер m) и нам нужно получить массив с размером **n+m**, для этого используем процедуру слияния.
#### Шаги процедуры слияния
1. Создаём новый массив **result** с размером **n+m**
2. Берём первый элемент из первого массива и первый элемент из второго массива и сравниваем их
3.
	- Если число из первого массива **меньше** ($<$) числа из второго массива, то число из первого массива записываем в **result**
	- Если число из первого массива **больше или равно** ($\geq$) числа из второго массива, то число из второго массива записываем в **result**
4. После записи меньшего элемента **переходим к следующему элементу** в том массиве, откуда взяли число. В другом массиве остаёмся на месте.
5. Повторяем процесс, пока один из массивов полностью не закончится
6. Если в одном из массивов **остались элементы**, просто дописываем их в **result**

Заметим, что при применении **процедуры слияния (merge)**
- Сложность количества действий  равна **$O(n+m)$**
- Сложность использования дополнительной памяти равна **$O(n+m)$**

> Стоит четко отличать функцию `merge` от функции `mergeSort`
>
> `mergeSort` - Получает 1 массив (возможно не отсортированный) и сортирует его
>
> `merge` - Получает 2 отсортированных массива и получает из них 1 сортированный массив.
>
> `mergeSort` использует `merge`
#### Рассмотрим псевдокод процедуры слияния (merge)
```cpp
vector merge(arr1, arr2)
{
    result = empty vector // массив размера n+m
    it1 = 0, it2 = 0 // индексы начала для первого и второго массива

    while it1 < size(arr1) && it2 < size(arr2)
    {
        if arr1[it1] < arr2[it2]
            result.push_back(arr1[it1++])
        else
            result.push_back(arr2[it2++])
	}

    // заполняем остальные элементы
    while it1 < size(arr1)
        result.push_back(arr1[it1++])

    while it2 < size(arr2)
        result.push_back(arr2[it2++])

    return result
}
```

#### Теперь рассмотрим псевдокод рекурсивного алгоритма Merge sort
```cpp
vector mergeSort(arr) {
    if size(arr) <= 1 // базовый случай
        return arr

    mid = size(arr) / 2
    left = mergeSort(arr[0:mid]) // левая часть
    right = mergeSort(arr[mid:end]) // правая часть

    return merge(left, right)
}

```

### Визуальное представление merge sort

При передаче функции `mergeSort` массива размера `n` сперва проводятся декствия

```cpp
    if size(arr) <= 1 // базовый случай
        return arr

    mid = size(arr) / 2
```
После чего происходит первый рекурсивный вызов

```cpp
    left = mergeSort(arr[0:mid]) // левая часть
```
То есть вызывается `mergeSort(arr[0:n/2])` и то же самое происходит рекурсивно пока условие `size(arr) <= 1` не станет справедливым. Ниже приведена демонстрация этого:

<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/mergeSortInDepth1.jpg" width="60%" />
</p>

Заметим, что в данный момент никакие 2 элемента небыли изменены местами, в массиве `arr` ничего не поменялось.

Как можно видеть, если условие `size(arr) <= 1` было справедливым, то происходит `return`, а возврат происходит на ту строку, откуда была вызвана функция

```cpp
    left = mergeSort(arr[0:mid])
```
после этого сработает строка
```cpp
    right = mergeSort(arr[mid:end]) // правая часть
```
То есть вызовется `mergeSort(arr[1:2]) `
<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/mergeSortInDepth2.jpg" width="60%" />
</p>

После чего опять же происходит `return` туда, откуда была вызвана функция
```cpp
    right = mergeSort(arr[mid:end]) // правая часть
```
И только после этого произойдет merge и сработает строка

```cpp
    return merge(left, right)
```

то есть будет вызвана функция `merge(arr[0:1], merge[1:2])` и функция вернет результат слияния двух массивов размера 1. То есть будут отсортированы первые 2 элемента в массиве
<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/mergeSortInDepth3.jpg" width="60%" />
</p>

### Примеры

1. Пример работы mergeSort в виде анимации.

Еще раз обратите свое внимание на то, что сортировка выполняется, когда происходит возвращение в предыдущий рекурсивный вызов.
<p align="center">
  <img src="https://willrosenbaum.com/assets/img/2022f-cosc-311/merge-sort.gif" />
</p>

2. Пример работы mergeSort в случае, когда размер массива не является степенью двойки
<p align="center">
  <img src="https://favtutor.com/resources/images/uploads/mceu_9916660651687944916761.png" />
</p>

3. Еще один пример в виде анимации (кликните, чтобы перейти по ссылке)

[![Video Title](https://i.ytimg.com/vi/ZRPoEKHXTJg/maxresdefault.jpg)](https://youtu.be/ZRPoEKHXTJg?si=QP6TaqoQJPj5lozJ)



### Время работы алгоритма
Чтобы оценить время работы этого алгоритма, составим рекуррентное соотношение, пусть **$T(n)$ - время сортировки массива длины n**.

Чтобы отсортировать массив длины $n$ алгоритм сперва сортирует 2 половины этого массива по отдельности, тратя на каждую из них по $T(\frac{n}{2})$ времени, а далее тратит $O(n)$ шагов на слияния результатов.

Тогда для **merge sort** справедливо равенство
$$T(n) = 2\cdot T(\frac n2) + O(n)$$
$O(n)$ - время, необходимое на то, чтобы слить два массива длины n

Распишем это соотношение:

$$T(n) = 2\cdot T(\frac {n}{2^1}) + O(n)$$
$$T(\frac {n}{2^1}) = 2\cdot T(\frac {n}{2^2}) + O(\frac {n}{2^1})$$
$$T(\frac {n}{2^2}) = 2\cdot T(\frac {n}{2^3}) + O(\frac {n}{2^2})$$
$$\vdots$$
$$T(\frac {n}{2^k}) = 1$$

Получили:

$$\frac {n}{2^k} \leq 1$$
$$2^k \geq n$$
$$k \geq \log_2n$$
А наименьшее целое число, которое больше либо равно $log_2 n$ равно
$$k = \lceil\log_2n\rceil$$

Умножим каждую часть соотношения на соответствующую степень 2, получим:

$$2^0\cdot T(n) = 2^1\cdot T(\frac {n}{2^1}) + O(n)$$
$$2^1\cdot T(\frac {n}{2^1}) = 2^2\cdot T(\frac {n}{2^2}) + O(n)$$
$$2^2\cdot T(\frac {n}{2^2}) = 2^3\cdot T(\frac {n}{2^3}) + O(n)$$
$$\vdots$$
$$2^k\cdot T(\frac {n}{2^k}) = 2^k \leq 2n$$

Сложим все эти равенства, получим:

$$T(n) \leq n \cdot(k+2) = n \cdot (\lceil\log_2n\rceil + 2) = O(n\log n)$$
$O(n\log n)$ $-$ сложность количества действий алгоритма **merge sort**, такую сложность называют линейно-логарифмической (это не только верхняя граница, но и нижняя)


Вопрос читателю: Можно ли оптимизировать данный код, чтобы программа занимала бы меньше памяти?

## Quick sort (быстрая сортировка)
### Принцип работы алгоритма
Алгоритм **quick sort** также использует принцип «разделяй и властвуй», процедура сортировки описывается следующим образом:
1. Если в рассматриваемом массиве один элемент, то он уже отсортирован — алгоритм завершает работу
2. Иначе выбираем **опорный элемент** (pivot)
3. Разделяем массив на две части используя **процедуру разбиения** (partition):
	- первый массив состоит из всех элементов **меньше опорного элемента** (pivot)
	- второй массив состоит из всех элементов **больше или равно опорного элемента** (pivot)
4. Рекурсивно сортируем обе части
5. Объединяем результаты
### Процедура разбиения (partition)
Предположим, у нас есть массив **a[l…r]** (**l** — индекс левого конца массива, **r** — индекс правого конца массива). Процедура разбиения изменяет расположение элементов в массиве так, что элементы слева от некоторого **опорного элемента** (pivot) меньше или равны этому значению, а элементы справа — больше или равны ему.
#### Шаги процедуры разбиения
1. Выбираем **опорный элемент** (pivot)
2. Используем два индекса:
	- **i** индекс левого конца массива (изначально **l**)
	- **j** индекс правого конца массива (изначально **r**)
3. Пока **low** $\leq$ **high**
	- Двигаем **low** вправо, пока не найдём элемент, который **больше опорного элемента** (pivot)
	- Двигаем **high** влево, пока не найдём элемент, который **меньше опорного элемента** (pivot)
	- Если **low** и **high** ещё не пересеклись, меняем местами (swap) найденные элементы
4. Как только индексы **low** и **high** пересекаются, процедура заканчивается.
#### Рассмотрим псевдокод процедуры разбиения (partition)
```cpp
def partition(arr, pivotIndex)
{
    pivot = arr[pivotIndex] // выбираем разделяющий элемент
    low = 0                 // индекс левого конца
    high = arr.size - 1     // индекс правого конца

    while low < high
    {
        // перемещаем левый индекс вправо, пока элемент меньше или равен разделяющему
        while low < high and arr[low] <= pivot
            ++low

        // перемещаем правый индекс влево, пока элемент больше или равен разделяющему
        while low < high and arr[high] >= v
            --high

        // если индексы не пересеклись, меняем элементы местами
        if low < high
        {
            swap(arr[low], arr[high])
            ++low
            --high
        }
    }

    return high // возвращаем новый индеь pivot
    // Можно было бы вернуть и low, так как цикл прекращается когда low == high
}

```

Рассмотрим анимацию partition на примере:

<p align="center">
  <img src="https://www.tutorialspoint.com/data_structures_algorithms/images/quick_sort_partition_animation.gif" />
</p>

#### Теперь рассмотрим псевдокод рекурсивного алгоритма quick sort
```cpp
def quicksort(arr)
{
    // Базовый случай: если подмассив содержит один или менее элементов, он уже отсортирован
    if arr.size() <= 1
        return

    // Выбираем pivot по какому-либо механизму (например самый первый элемент в массиве)
    initialPivotIndex = pivot(arr)

    // вызываем функцию partition, чтобы найти индекс разделяющего элемента
    pivotIndex = partition(arr, initialPivotIndex)

    // рекурсивно сортируем левую часть массива
    quicksort(arr[0 : pivotIndex])

    // рекурсивно сортируем правую часть массива
    quicksort(arr[pivotIndex + 1 : arr.size()])
}

```

#### Рассмотрим пример работы quickSorrt в виде анимации

Тут `pivot` выбирался случайным образом, то есть в этом примере функция `pivot(arr)` каждый раз возвращалa случайный индекс в диапозоне `[0, arr.size)`
<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/quickSortAnimation.gif" />
</p>

> `quickSort`, в отличае от `mergeSort`, сортирует массив при спуске, а не при возвращении.

### Время работы quickSort в худшем случае
#### Худшее время работы
Рассмотрим случай, когда разбиение массива происходит неравномерно: одна часть содержит **$n−1$** элементов, а другая содержит лишь 1 элемент.

Так как процедура разбиения выполняется за **$Θ(n)$**, для времени работы **$T(n)$** получаем рекуррентное соотношение:

$$
T(n) = T(n-1) + O(n) = \sum_{k=1}^n Θ(k) = Θ\left(\sum_{k=1}^nk\right) = Θ(n^2)
$$

Такой случай возникает, если **опорный элемент** (pivot) каждый раз после **процедуры разбиения** (partition) оказывается либо в начале, либо в конце массива, что делает разбиение неэффективным.
#### Среднее время работы
Среднее время работы алгоритма **quick sort** равно $O(n\log n)$, такой сложности мы достигаем, если **опорный элемент** (pivot) падает на середину (или почти середину) массива.
### Сравнение с bubble sort
Рассмотрим таблицу:

|  Алгоритм   | Сложность в лучшем случае | Сложность в худшем случае |
|:-----------:|:-------------------------:|:-------------------------:|
| Quick sort  |       **$n\log n$**       |         **$n^2$**         |
| Bubble sort |          **$n$**          |         **$n^2$**         |

По данной таблице можно подумать, что bubble sort лучше quick sort, но у quick sort в среднем худший случай случается реже

### Амортизированная сложность
Quick sort амортизированно работает за **$O(n \log n)$**
Bubble sort амортизированно работает за **$O(n^2)$**

Таким образом приходим к следующей таблице
|  Алгоритм   | Сложность в лучшем случае | Сложность в худшем случае |Амортизированная сложность | Дополнительная память |
|:-----------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|
| Merge sort  |    **$O(n\log n)$**       |    **$O(n\log n)$**       |      **$O(n\log n)$**     |       **$O(n)$**     |
| Quick sort  |    **$O(n\log n)$**       |      **$O(n^2)$**         |      **$O(n\log n)$**     |       **$O(1)$**     |
| Bubble sort |       **$O(n)$**          |      **$O(n^2)$**         |      **$O(n^2$)**         |      **$O(1)$**         |

## Heap sort (сортировка кучей)
### Принцип работы алгоритма
Алгоритм **Heap Sort** работает через структуру данных **куча** и использует принцип **сортировки выбором**.
Мы будем использовать кучу для того, чтобы в процессе сортировки на каждом шаге извлекать наибольший элемент и ставить его на своё место в отсортированной части массива, процедура сортировки описывается следующим образом:
1. Строим из массива кучу с помощью **make_heap** (сложность $O(n)$)
2. Пока куча не станет пустой делаем **pop_heap** (сложность $O(\log n)$)

Рассмотрим код **heap sort**
```cpp
#include <algorithm> // Для make_heap и pop_heap

void heapSort(std::vector<int> arr)
{
    // Строим пирамиду.
    make_heap(arr.begin(), arr.end()); // О(n)

    // Процесс сортировки
    for (std::size_t i = arr.size(); i > 0; --i) {
        // Функция pop_heap меняет местами arr[0] и arr[i-1]
        // и делает так, чтобы подмассив arr[0:i-1) стал бы heap-ом.
        // Таким образом максимальный элемент подмассива arr[0: i) будет
        // на месте arr[i-1] в конце каждой итерации.
        pop_heap(arr.begin(), arr.begin() + i); //O(logn)
    }
}
```
### Время работы алгоритма
- `make_heap` происходит за **$O(n)$** времени, а `pop_heap` выполняется $n$ раз каждый за $O(\log n)$ времени. Совместив их получаем $O(n + n \log n) = O(n\log n)$
- Сложность использования дополнительной памяти **$O(1)$**
Заметим, недостаток **heap sort**:
На почти отсортированных данных данный алгоритм работает столь же долго, как на несортированных

### Код heap sort
```cpp
#include <vector>
#include <algorithm>

// Функция для построения кучи и сортировки
void heapsort(std::vector<int>& arr) {
    // Преобразуем массив в кучу
    std::make_heap(arr.begin(), arr.end());

    // Извлекаем элементы из кучи и сортируем
    for (auto i = arr.end(); i != arr.begin(); --i) {
        std::pop_heap(arr.begin(), i);
    }
}
```

### Визуальное представление heap sort
Повторим способ создания `heap` из вектора, который мы прошли на предыдущем семестре, приведя пример работы `make_heap` в виде анимации

<p align="center">
  <img src="https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fdpy879r83nua30a2ntgf.gif" />
</p>

Также повторим работу одного вызова `pop_heap` при помощи операции
<p align="center">
  <img src="https://www.tutorialspoint.com/data_structures_algorithms/images/max_heap_deletion_animation.gif" />
</p>

# Lower bound for comparison based sorting with the proof
## Сортировка сравнениями
**Сортировка сравнениями** $-$ алгоритм сортировки, который совершает операции сравнения элементов, но никак не использует их внутреннюю структуру.
## Теорема (о нижней оценке для сортировки сравнениями):
В худшем случае любой алгоритм сортировки сравнениями выполняет $\Omega(n\log n)$ действий, где $n$ $-$ число сортируемых элементов.
### Доказательство:
Любой алгоритм сортировки сравнениями можно представить в виде **дерева выбора**:

<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/DecisionTree1.png" />
</p>

Пример:
Пусть имеем массив с элементами $({a[0],a[1],a[2]})$, нарисуем для него **плохое дерево выбора**

<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/DecisionTree2.png" />
</p>

При сравнении элементов заметим, что возможно два исхода, значит, у каждого узла есть не более двух сыновей, всего существует **$n!$** различных перестановок **$n$** элементов, значит, число листьев дерева не менее $n!$

Красным цветом отмечена часть, которая не имеет смысла (делает дерево выбора плохим)
Заметим, что в дереве выбора, количество сравнений в худшем случае может быть равна высоте дерева

#### Утверждение:
Если в бинарном дереве **$L$** листьев, то **$L \leq 2^h$**
##### Доказательство:
Докажем методом математической индукции:
Пусть **$h=0$**, тогда получим **$1 \leq 2^0$** и **$0 \leq 2^0$**
Предположим, что неравенство верно для **$h$**
Докажем для **$h+1$**:

<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/DecisionTree3.png" />
</p>

Если у нашего узла два поддерева и наша общая высота равна **$h+1$**, то мы точно можем сказать, что высота одного из поддеревьев равна **$h$**, а высота другого **$\leq h$**
Допустим в нашем случае
- Высота левого поддерева **$h_1 = h$**
- Высота правого поддерева **$h_2 \leq h$**
То есть высота дерева не больше, чем **$2 \cdot 2^h = 2^{h+1}$**
Получили неравенство **$L \leq 2^{h+1}$** $\implies$ доказали, что **$L \leq 2^h$**

Из данного утверждения следует, что $$n! \leq 2^h$$ $$h \geq \log_2 n! = \log_2 (1 \cdot 2 \cdot 3 \dotso \cdot [\frac n2] \cdot ([\frac n2] + 1) \cdot \dotso \cdot n) \geq \log_2 (\frac n2)^\frac n2 = \frac n2 \log_2 \frac n2 = \Omega(n\log n)$$
В итоге доказали, что никакой алгоритм сортировки сравнениями в худшем случае не может быть быстрее **$O(n\log n)$**