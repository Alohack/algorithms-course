# Sorting algorithms and their complexities

## Merge sort (сортировка слиянием)
### Принцип работы алгоритма
Алгоритм **merge sort** использует принцип «разделяй и властвуй», процедура сортировки описывается следующим образом:
1. Если в рассматриваемом массиве один элемент, то он уже отсортирован — алгоритм завершает работу
2. Иначе массив разбивается на две части, которые сортируются рекурсивно
3. После сортировки двух частей массива к ним применяется **процедура слияния**, которая по двум отсортированным частям получает исходный отсортированный массив

### Процедура слияния (merge)
Пусть имеем два массива **arr1** (размер n) и **arr2** (размер m) и нам нужно получить массив с размером **n+m**, для этого используем процедуру слияния.
#### Шаги процедуры слияния
1. Создаём новый массив **result** с размером **n+m**
2. Берём первый элемент из первого массива и первый элемент из второго массива и сравниваем их
3.
	- Если число из первого массива **меньше** ($<$) числа из второго массива, то число из первого массива записываем в **result**
	- Если число из первого массива **больше или равно** ($\geq$) числа из второго массива, то число из второго массива записываем в **result**
4. После записи меньшего элемента **переходим к следующему элементу** в том массиве, откуда взяли число. В другом массиве остаёмся на месте.
5. Повторяем процесс, пока один из массивов полностью не закончится
6. Если в одном из массивов **остались элементы**, просто дописываем их в **result**

Заметим, что при применении **процедуры слияния (merge)**
- Сложность количества действий  равна **$O(n+m)$**
- Сложность использования дополнительной памяти равна **$O(n+m)$**

> Стоит четко отличать функцию `merge` от функции `mergeSort`
>
> `mergeSort` - Получает 1 массив (возможно не отсортированный) и сортирует его
>
> `merge` - Получает 2 отсортированных массива и получает из них 1 сортированный массив.
>
> `mergeSort` использует `merge`
#### Рассмотрим псевдокод процедуры слияния (merge)
```cpp
vector merge(arr1, arr2)
{
    result = empty vector // массив размера n+m
    it1 = 0, it2 = 0 // индексы начала для первого и второго массива

    while it1 < size(arr1) && it2 < size(arr2)
    {
        if arr1[it1] < arr2[it2]
            result.push_back(arr1[it1++])
        else
            result.push_back(arr2[it2++])
	}

    // заполняем остальные элементы
    while it1 < size(arr1)
        result.push_back(arr1[it1++])

    while it2 < size(arr2)
        result.push_back(arr2[it2++])

    return result
}
```

#### Теперь рассмотрим псевдокод рекурсивного алгоритма Merge sort
```cpp
vector mergeSort(arr) {
    if size(arr) <= 1 // базовый случай
        return arr

    mid = size(arr) / 2
    left = mergeSort(arr[0:mid]) // левая часть
    right = mergeSort(arr[mid:end]) // правая часть

    return merge(left, right)
}

```

### Визуальное представление merge sort

При передаче функции `mergeSort` массива размера `n` сперва проводятся декствия

```cpp
    if size(arr) <= 1 // базовый случай
        return arr

    mid = size(arr) / 2
```
После чего происходит первый рекурсивный вызов

```cpp
    left = mergeSort(arr[0:mid]) // левая часть
```
То есть вызывается `mergeSort(arr[0:n/2])` и то же самое происходит рекурсивно пока условие `size(arr) <= 1` не станет справедливым. Ниже приведена демонстрация этого:

<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/mergeSortInDepth1.jpg" width="60%" />
</p>

Заметим, что в данный момент никакие 2 элемента небыли изменены местами, в массиве `arr` ничего не поменялось.

Как можно видеть, если условие `size(arr) <= 1` было справедливым, то происходит `return`, а возврат происходит на ту строку, откуда была вызвана функция

```cpp
    left = mergeSort(arr[0:mid])
```
после этого сработает строка
```cpp
    right = mergeSort(arr[mid:end]) // правая часть
```
То есть вызовется `mergeSort(arr[1:2]) `
<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/mergeSortInDepth2.jpg" width="60%" />
</p>

После чего опять же происходит `return` туда, откуда была вызвана функция
```cpp
    right = mergeSort(arr[mid:end]) // правая часть
```
И только после этого произойдет merge и сработает строка

```cpp
    return merge(left, right)
```

то есть будет вызвана функция `merge(arr[0:1], merge[1:2])` и функция вернет результат слияния двух массивов размера 1. То есть будут отсортированы первые 2 элемента в массиве
<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/mergeSortInDepth3.jpg" width="60%" />
</p>

### Примеры

1. Пример работы mergeSort в виде анимации.

Еще раз обратите свое внимание на то, что сортировка выполняется, когда происходит возвращение в предыдущий рекурсивный вызов.
<p align="center">
  <img src="https://willrosenbaum.com/assets/img/2022f-cosc-311/merge-sort.gif" />
</p>

2. Пример работы mergeSort в случае, когда размер массива не является степенью двойки
<p align="center">
  <img src="https://favtutor.com/resources/images/uploads/mceu_9916660651687944916761.png" />
</p>

3. Еще один пример в виде анимации (кликните, чтобы перейти по ссылке)

[![Video Title](https://i.ytimg.com/vi/ZRPoEKHXTJg/maxresdefault.jpg)](https://youtu.be/ZRPoEKHXTJg?si=QP6TaqoQJPj5lozJ)



### Время работы алгоритма
Чтобы оценить время работы этого алгоритма, составим рекуррентное соотношение, пусть **$T(n)$ - время сортировки массива длины n**.

Чтобы отсортировать массив длины $n$ алгоритм сперва сортирует 2 половины этого массива по отдельности, тратя на каждую из них по $T(\frac{n}{2})$ времени, а далее тратит $O(n)$ шагов на слияния результатов.

Тогда для **merge sort** справедливо равенство
$$T(n) = 2\cdot T(\frac n2) + O(n)$$
$O(n)$ - время, необходимое на то, чтобы слить два массива длины n

Распишем это соотношение:

$$T(n) = 2\cdot T(\frac {n}{2^1}) + O(n)$$
$$T(\frac {n}{2^1}) = 2\cdot T(\frac {n}{2^2}) + O(\frac {n}{2^1})$$
$$T(\frac {n}{2^2}) = 2\cdot T(\frac {n}{2^3}) + O(\frac {n}{2^2})$$
$$\vdots$$
$$T(\frac {n}{2^k}) = 1$$

Получили:

$$\frac {n}{2^k} \leq 1$$
$$2^k \geq n$$
$$k \geq \log_2n$$
А наименьшее целое число, которое больше либо равно $log_2 n$ равно
$$k = \lceil\log_2n\rceil$$

Умножим каждую часть соотношения на соответствующую степень 2, получим:

$$2^0\cdot T(n) = 2^1\cdot T(\frac {n}{2^1}) + O(n)$$
$$2^1\cdot T(\frac {n}{2^1}) = 2^2\cdot T(\frac {n}{2^2}) + O(n)$$
$$2^2\cdot T(\frac {n}{2^2}) = 2^3\cdot T(\frac {n}{2^3}) + O(n)$$
$$\vdots$$
$$2^k\cdot T(\frac {n}{2^k}) = 2^k \leq 2n$$

Сложим все эти равенства, получим:

$$T(n) \leq n \cdot(k+2) = n \cdot (\lceil\log_2n\rceil + 2) = O(n\log n)$$
$O(n\log n)$ $-$ сложность количества действий алгоритма **merge sort**, такую сложность называют линейно-логарифмической (это не только верхняя граница, но и нижняя)


Вопрос читателю: Можно ли оптимизировать данный код, чтобы программа занимала бы меньше памяти?

## Quick sort (быстрая сортировка)
### Принцип работы алгоритма
Алгоритм **quick sort** также использует принцип «разделяй и властвуй», процедура сортировки описывается следующим образом:
1. Если в рассматриваемом массиве один элемент, то он уже отсортирован — алгоритм завершает работу
2. Иначе выбираем **опорный элемент** (pivot)
3. Разделяем массив на две части используя **процедуру разбиения** (partition):
	- первый массив состоит из всех элементов **меньше опорного элемента** (pivot)
	- второй массив состоит из всех элементов **больше или равно опорного элемента** (pivot)
4. Рекурсивно сортируем обе части
5. Объединяем результаты
### Процедура разбиения (partition)
Предположим, у нас есть массив **a[l…r]** (**l** — индекс левого конца массива, **r** — индекс правого конца массива). Процедура разбиения изменяет расположение элементов в массиве так, что элементы слева от некоторого **опорного элемента** (pivot) меньше или равны этому значению, а элементы справа — больше или равны ему.
#### Шаги процедуры разбиения
1. Выбираем **опорный элемент** (pivot)
2. Используем два индекса:
	- **low** индекс левого конца массива (изначально **0**)
	- **high** индекс правого конца массива (изначально **arr.size - 1**)
3. Пока **low** $\leq$ **high**
	- Двигаем **low** вправо, пока не найдём элемент, который **больше опорного элемента** (pivot)
	- Двигаем **high** влево, пока не найдём элемент, который **меньше опорного элемента** (pivot)
	- Если **low** и **high** ещё не пересеклись, меняем местами (swap) найденные элементы
4. Как только индексы **low** и **high** пересекаются, процедура заканчивается.
#### Рассмотрим псевдокод процедуры разбиения (partition)
```cpp
def partition(arr, pivotIndex)
{
    pivot = arr[pivotIndex] // выбираем разделяющий элемент
    low = 0                 // индекс левого конца
    high = arr.size - 1     // индекс правого конца

    while low < high
    {
        // перемещаем левый индекс вправо, пока элемент меньше или равен разделяющему
        while low < high and arr[low] <= pivot
            ++low

        // перемещаем правый индекс влево, пока элемент больше или равен разделяющему
        while low < high and arr[high] >= v
            --high

        // если индексы не пересеклись, меняем элементы местами
        if low < high
        {
            swap(arr[low], arr[high])
            ++low
            --high
        }
    }

    return high // возвращаем новый индекс pivot
    // Можно было бы вернуть и low, так как цикл прекращается когда low == high
}

```

Рассмотрим анимацию partition на примере:

<p align="center">
  <img src="https://www.tutorialspoint.com/data_structures_algorithms/images/quick_sort_partition_animation.gif" />
</p>

#### Теперь рассмотрим псевдокод рекурсивного алгоритма quick sort
```cpp
def quicksort(arr)
{
    // Базовый случай: если подмассив содержит один или менее элементов, он уже отсортирован
    if arr.size() <= 1
        return

    // Выбираем pivot по какому-либо механизму (например самый первый элемент в массиве)
    initialPivotIndex = pivot(arr)

    // вызываем функцию partition, чтобы найти индекс разделяющего элемента
    pivotIndex = partition(arr, initialPivotIndex)

    // рекурсивно сортируем левую часть массива
    quicksort(arr[0 : pivotIndex])

    // рекурсивно сортируем правую часть массива
    quicksort(arr[pivotIndex + 1 : arr.size()])
}

```

#### Рассмотрим пример работы quickSorrt в виде анимации

Тут `pivot` выбирался случайным образом, то есть в этом примере функция `pivot(arr)` каждый раз возвращалa случайный индекс в диапозоне `[0, arr.size)`
<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/quickSortAnimation.gif" />
</p>

> `quickSort`, в отличае от `mergeSort`, сортирует массив при спуске, а не при возвращении.

### Время работы quickSort в худшем случае
#### Худшее время работы
Рассмотрим случай, когда разбиение массива происходит неравномерно: одна часть содержит **$n−1$** элементов, а другая содержит лишь 1 элемент.

Так как процедура разбиения выполняется за **$Θ(n)$**, для времени работы **$T(n)$** получаем рекуррентное соотношение:

$$
T(n) = T(n-1) + O(n) = \sum_{k=1}^n Θ(k) = Θ\left(\sum_{k=1}^nk\right) = Θ(n^2)
$$

Такой случай возникает, если **опорный элемент** (pivot) каждый раз после **процедуры разбиения** (partition) оказывается либо в начале, либо в конце массива, что делает разбиение неэффективным.

Но если же `pivot` будет падать на середину или почти на середину, то сложность будет такая же как и у `mergeSort` - $O(n\log n)$

### Сравнение с bubble sort
Рассмотрим таблицу:

|  Алгоритм   | Сложность в лучшем случае | Сложность в худшем случае |
|:-----------:|:-------------------------:|:-------------------------:|
| Quick sort  |       **$n\log n$**       |         **$n^2$**         |
| Bubble sort |          **$n$**          |         **$n^2$**         |

По данной таблице можно подумать, что bubble sort лучше quick sort, но у quick sort в среднем худший случай случается реже

### Амортизированная сложность

Обозначим через $T(n)$ среднее по вероятности количество сравнений элементов массива размера $n$ во время выполнения алгоритма `quickSort` (это называется математическим ожиданием).

> Утверждение: $T(n) \leq 2n\log n, \text{при } n \geq 1$.

**Доказательство**

Доказывать будем по индукции. <br>
Очевидно, что когда $n=1$ то алгоритм `quickSort` не выполнит сравнений вовсе, то есть $T(1)=0 \leq 1\log1$.<br>
Ну и тем более очевидно, что $T(0)=0$.

Предположим, что утверждение верно при всех натуральных чисел меньше чем $n$. <br>
Докажем для $n$.

Во время алгоритма `quickSort` сперва выбирается `pivot` и он может попасть в любое место массива с равной вероятностью - $\frac{1}{n}$.

Обозначим через $k$ число элементов слева от `pivot` после `partition` и заметим, что если $k$ фиксировано, то алгоритм выполнит

1. $(n-1)$ сравнений во время `partition`
2. $T(k)$ сравнений во время сортировки левого подмассива
3. $T(n-1-k)$ сравнений во время сортировки правого подмассива

<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/QuickSortAmort.jpg" width="50%" />
</p>

Так как выпадение `pivot` на любое место равновероятно, то значение $k = 0, 1, 2, ..., n-1$ также равновероятно.<br>
Следовательно среднее число операций (мат. ожидание числа операций) для левого подмассива $\frac{1}{n} \sum\limits_{k=0}^{n-1} T(k)$. <br>
A для правого подмассива $\frac{1}{n} \sum\limits_{k=0}^{n-1} T(n-k-1)$. 

В итоге имеем

$$
T(n) = (n-1) + \frac{1}{n} \sum\limits_{k=0}^{n-1} T(k) + \frac{1}{n} \sum\limits_{k=0}^{n-1} T(n-k-1) = (n-1) + \frac{2}{n} \sum\limits_{k=0}^{n-1} T(k)
$$

Так как согласно предположению индукции $T(k) \leq 2k \log k$ при $k=1, ..., n-1$,

$$
T(n) = (n-1) + \frac{2}{n} \sum\limits_{k=1}^{n-1} T(k) \leq (n-1) + \frac{2}{n} \sum\limits_{k=1}^{n-1} 2k\log k
$$

Заметим, что изначально сумма начиналась с нуля, но так как $T(0) = 0$ теперь она начинается с $k=1$.

Теперь же рассмотрим функцию $y=x\log x, x \geq 1$. Из за того, что эта функция монотонно возрастает, можно утверждать, что

$$
\sum\limits_{k=1}^{n-1} k\log k \leq \int\limits_1^n x\log x  dx.
$$

В этом можно легко убедится на графике, в котором видно, что площадь, под графиком $x \log x$ больше, чем сумма площадей прямоугольников с высотами $k \log k$ и шириной $1$.

<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/QuickSortAmort2.jpg" width="70%"/>
</p>

В итоге синтегрировав по частям имеем, что 

$$
T(n) \leq n-1 + \frac{2}{n} \int\limits_1^n 2x\log x  dx = n-1 +  \frac{2}{n}\int\limits_1^n \log x  dx^2 = n-1 + x^2 \log x \Big|_1^n - \frac{2}{n}\int\limits_1^n x^2  d \log x =
$$

$$
= n-1 +  \frac{2}{n}n^2 \log n  - \frac{2}{n}\int\limits_1^n x dx = n-1 + 2n\log n - \frac{1}{n}(n^2-1) \leq 2n \log n
$$

В итоге имеем, что

Quick sort амортизированно работает за **$O(n \log n)$** <br>
Bubble sort амортизированно работает за **$O(n^2)$**

Таким образом приходим к следующей таблице
|  Алгоритм   | Сложность в лучшем случае | Сложность в худшем случае |Амортизированная сложность | Дополнительная память |
|:-----------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|
| Merge sort  |    **$O(n\log n)$**       |    **$O(n\log n)$**       |      **$O(n\log n)$**     |       **$O(n)$**     |
| Quick sort  |    **$O(n\log n)$**       |      **$O(n^2)$**         |      **$O(n\log n)$**     |       **$O(1)$**     |
| Bubble sort |       **$O(n)$**          |      **$O(n^2)$**         |      **$O(n^2$)**         |      **$O(1)$**         |

## Heap sort (сортировка кучей)
### Принцип работы алгоритма
Алгоритм **Heap Sort** работает через структуру данных **куча** и использует принцип **сортировки выбором**.
Мы будем использовать кучу для того, чтобы в процессе сортировки на каждом шаге извлекать наибольший элемент и ставить его на своё место в отсортированной части массива, процедура сортировки описывается следующим образом:
1. Строим из массива кучу с помощью **make_heap** (сложность $O(n)$)
2. Пока куча не станет пустой делаем **pop_heap** (сложность $O(\log n)$)

Рассмотрим код **heap sort**
```cpp
#include <algorithm> // Для make_heap и pop_heap

void heapSort(std::vector<int> arr)
{
    // Строим пирамиду.
    make_heap(arr.begin(), arr.end()); // О(n)

    // Процесс сортировки
    for (std::size_t i = arr.size(); i > 0; --i) {
        // Функция pop_heap меняет местами arr[0] и arr[i-1]
        // и делает так, чтобы подмассив arr[0:i-1) стал бы heap-ом.
        // Таким образом максимальный элемент подмассива arr[0: i) будет
        // на месте arr[i-1] в конце каждой итерации.
        pop_heap(arr.begin(), arr.begin() + i); //O(logn)
    }
}
```
### Время работы алгоритма
- `make_heap` происходит за **$O(n)$** времени, а `pop_heap` выполняется $n$ раз каждый за $O(\log n)$ времени. Совместив их получаем $O(n + n \log n) = O(n\log n)$
- Сложность использования дополнительной памяти **$O(1)$**
Заметим, недостаток **heap sort**:
На почти отсортированных данных данный алгоритм работает столь же долго, как на несортированных

### Код heap sort
```cpp
#include <vector>
#include <algorithm>

// Функция для построения кучи и сортировки
void heapsort(std::vector<int>& arr) {
    // Преобразуем массив в кучу
    std::make_heap(arr.begin(), arr.end());

    // Извлекаем элементы из кучи и сортируем
    for (auto i = arr.end(); i != arr.begin(); --i) {
        std::pop_heap(arr.begin(), i);
    }
}
```

### Визуальное представление heap sort
Повторим способ создания `heap` из вектора, который мы прошли на предыдущем семестре, приведя пример работы `make_heap` в виде анимации

<p align="center">
  <img src="https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fdpy879r83nua30a2ntgf.gif" />
</p>

Также повторим работу одного вызова `pop_heap` при помощи операции
<p align="center">
  <img src="https://www.tutorialspoint.com/data_structures_algorithms/images/max_heap_deletion_animation.gif" />
</p>

# Нижняя оценка сортировки сравнением
## Сортировка сравнениями
**Сортировка сравнениями** $-$ алгоритм сортировки, который совершает операции сравнения элементов, но никак не использует их внутреннюю структуру.
## Теорема (о нижней оценке для сортировки сравнениями):
В худшем случае любой алгоритм сортировки сравнениями выполняет $\Omega(n\log n)$ действий, где $n$ $-$ число сортируемых элементов.
### Доказательство:
Любой алгоритм сортировки сравнениями можно представить в виде **дерева выбора**:

<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/DecisionTree1.png" />
</p>

Пример:
Пусть имеем массив с элементами $({a[0],a[1],a[2]})$, нарисуем для него **плохое дерево выбора**

<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/DecisionTree2.png" />
</p>

При сравнении элементов заметим, что возможно два исхода, значит, у каждого узла есть не более двух сыновей, всего существует **$n!$** различных перестановок **$n$** элементов, значит, число листьев дерева не менее $n!$

Красным цветом отмечена часть, которая не имеет смысла (делает дерево выбора плохим)
Заметим, что в дереве выбора, количество сравнений в худшем случае может быть равна высоте дерева

#### Утверждение:
Если в бинарном дереве **$L$** листьев, то **$L \leq 2^h$**
##### Доказательство:
Докажем методом математической индукции:
Пусть **$h=0$**, тогда получим **$1 \leq 2^0$** и **$0 \leq 2^0$**
Предположим, что неравенство верно для **$h$**
Докажем для **$h+1$**:

<p align="center">
  <img src="https://raw.githubusercontent.com/Alohack/algorithms-course/refs/heads/main/Images/DecisionTree3.png" />
</p>

Если у нашего узла два поддерева и наша общая высота равна **$h+1$**, то мы точно можем сказать, что высота одного из поддеревьев равна **$h$**, а высота другого **$\leq h$**
Допустим в нашем случае
- Высота левого поддерева **$h_1 = h$**
- Высота правого поддерева **$h_2 \leq h$**
То есть высота дерева не больше, чем **$2 \cdot 2^h = 2^{h+1}$**
Получили неравенство **$L \leq 2^{h+1}$** $\implies$ доказали, что **$L \leq 2^h$**

Из данного утверждения следует, что 

$$
n! \leq 2^h
$$ 

А следовательно

$$
h \geq \log_2 n! = \log_2 (1 \cdot 2 \cdot 3 \dotso \cdot [\frac n2] \cdot ([\frac n2] + 1) \cdot \dotso \cdot n) \geq \log_2 (([\frac n2] + 1) \cdot \dotso \cdot n) \geq \log_2 (\frac n2)^\frac n2 = \frac n2 \log_2 \frac n2 = \Omega(n\log n)
$$

В итоге доказали, что никакой алгоритм сортировки сравнениями в худшем случае не может быть быстрее **$O(n\log n)$**

# CountSort


Пусть имеем массив размера $n$, который состоит из целых чисел в диапозоне $[0, m)$.

Алгоритм `CountSort` заключается в следующем

1. Наряду с массивом `arr` размера `n` из чисел, кторые мы должны отсортировать, объявим еще один массив `counts` размера `m`.
2. Приравняем все значения массива `counts` к 0.
3. Пройдемся по всему массиву `arr` и пусть `x` очередной элемент массива `arr`, тогда выполним `++counts[x]`
4. После того как прошлись по всему массиву `arr`, проходим по массиву `counts` и приравниваем очередные
   `counts[y]` элементов массива `arr` к `y`

Код оставляется в качестве упражнения для читателя. Ниже можете найти анимацию к `countSort` (сверху массив `arr`, снизу массив `counts`) 

<p align="center">
  <img src="https://res.cloudinary.com/codecrucks/images/c_scale,w_640,h_352/f_webp,q_auto/v1631289534/counting-sort/counting-sort.gif?_i=AA" />
</p>

Таким образом на шаге `3` происходит $O(n)$ действий так как `arr.size = n`, а на шаге `4` $m$ действий, так как `counts.size = m`. Итого $O(n + m)$ действий.

> Вопрос: А не противоречит ли это теореме, которую мы доказали ранее? Ведь фактически мы отсортировали массив за линейное время, а наименьшее время для сортировки массива ведь линейно-логарифмическое согласно теореме?

Ответ: Не противоречит, так как при сортировке `CountSort` у нас есть дополнительное условие, что все элементы массива целые числа и находятся в диапозоне $[0, m)$ и мы пользуемся этим условием при сортировке, а в теореме это не предполагается.

Расширенный ответ: Представим случай, когда $m=1$, тогда мы можем отсортировать массив даже не за $O(n)$, а за $O(1)$, так как в этом случае в массиве все элементы равны друг другу и мы просто вернем тот массив который получили.

> Вопрос: Что меняется в дереве выбора при этом подходе?

Рассмотрим на примере самого простого случая - когда $m=1$. <br>
В дереве выбора теперь будет не $n!$ листьев, а всего $1$ лист, ведь если все эллементы равны друг другу, то есть всего одна возможность их сортировки.

В итоге получаем такую таблицу


Таким образом приходим к следующей таблице
|  Алгоритм   | Сложность в лучшем случае | Сложность в худшем случае |Амортизированная сложность | Дополнительная память |
|:-----------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|
| Merge sort  |    **$O(n\log n)$**       |    **$O(n\log n)$**       |      **$O(n\log n)$**     |       **$O(n)$**        |
| Quick sort  |    **$O(n\log n)$**       |      **$O(n^2)$**         |      **$O(n\log n)$**     |       **$O(1)$**        |
| Heap sort   |    **$O(n\log n)$**       |      **$O(n\log n)$**     |      **$O(n\log n)$**     |       **$O(1)$**        |
| Bubble sort |       **$O(n)$**          |      **$O(n^2)$**         |      **$O(n^2$)**         |      **$O(1)$**         |
| Count sort  |    **$O(n + m)$**         |      **$O(n + m)$**       |      **$O(n + m)$**       |       **$O(m)$**        |